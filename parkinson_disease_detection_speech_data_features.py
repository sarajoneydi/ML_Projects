# -*- coding: utf-8 -*-
"""PARKINSON_HW01sarajoneydi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13h93y4bj1eFNrDR2lwvhShKrvJwkOmfU
"""

# ML. HW01.PARKINSON DISEASE DATASET ANALYSIS, 

# Sara Joneydi
# Date loaded in elearn: جمعه بیست و ششم دیماه 1399

# Commented out IPython magic to ensure Python compatibility.
## کتابخانه و پکیج های مورد استفاده

# %matplotlib inline
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt
import matplotlib.image as pltimg
import seaborn as sns
import random 
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score,recall_score,f1_score
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from tqdm import tqdm_notebook as tqdm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score

df = pd.read_csv('pd_speech_feature.csv')
df.head()

df.info()

df.describe()

df.isna().sum()

# کنترل اینکه گروه بندی با برچسب کلاس 0و1 به ما 188 بیمار و 64 سالم می دهد. تقسیم بر 3 زیرا هر فرد 3 بار ضبط انجام داده
df['patient/healthy count'] = 1
df.groupby('class').sum()/3

pd_speech_features = df.drop(['patient/healthy count'], axis = 1)

# نوع داده ها در دیتافریم 
pd_speech_features =  pd_speech_features.astype(float) #per default all floats 
pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']] = pd_speech_features[['id', 'numPulses', 'numPeriodsPulses']].astype(int) #ints
pd_speech_features[['gender', 'class']] = pd_speech_features[['gender', 'class']].astype('category') #categoricals
pd_speech_features.dtypes

"""**Correlation Matrics and Dimension Reduction**"""

#از ماتریس همبستگی می توان دریافت که برخی از ویژگی ها به شدت با یکدیگر ارتباط دارند. به منظور کاهش ابعاد مسئله ، حذف یکی از دو ویژگی به شدت وابسته (مثبت یا منفی) را انجام میدهم.
corr = pd_speech_features.corr() 
sns.heatmap(np.abs(corr), 
        xticklabels=[],
        yticklabels=[])

plt.figure(figsize=(10,10))
sns.heatmap(pd_speech_features.corr());

# حذف ویژگی های بشدت وابست
#مانند مقاله حذف ویژگی tqwm
pd_speech_features_no_tqwt = pd_speech_features[pd_speech_features.columns[0: -433]]
pd_speech_features_no_tqwt.head()

correlation_values=df.corr()['class']
correlation_values.abs().sort_values(ascending=False)

features=df.loc[:,df.columns!='class']
labels=df.loc[:,'class']

print("Samples with label '0': {}\n Samples with label '1': {}".format(labels[labels==0].shape, labels[labels==1].shape))

"""**XGBoost Classifier**"""

mm_scaler = MinMaxScaler(feature_range=(-1,1))

X = mm_scaler.fit_transform(features.drop('id', axis=1)) 
y = labels.values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7)

xbg_model = XGBClassifier()
xbg_model.fit(X_train, y_train)

y_pred = xbg_model.predict(X_test)

print(confusion_matrix(y_test, y_pred))

print(accuracy_score(y_test, y_pred))

print(precision_score(y_test, y_pred))

print(recall_score(y_test, y_pred))

print(f1_score(y_test, y_pred))

##حذف داده های پرت 
Q1 = np.percentile(pd_speech_features.numPulses, 25)
Q3 = np.percentile(pd_speech_features.numPulses, 75)
    
# پیدا کردن کران مجاز
IQR = Q3 - Q1
low_outlier = max(0, Q1 - 3*IQR)
high_outlier = Q3 + 3*IQR 

print( "محدوده مجاز: [", low_outlier , high_outlier, ']')

# Filter 
df_no_outliers = pd_speech_features[(pd_speech_features.numPulses < high_outlier) | (pd_speech_features.numPulses > low_outlier)]
df_out_outliers = pd_speech_features[(pd_speech_features.numPulses > high_outlier) | (pd_speech_features.numPulses < low_outlier)]

# نمایش پراکندگی
ax =  df_no_outliers.plot.scatter(x='id', y='numPulses', label = 'numPulses')
df_out_outliers.plot.scatter(x='id', y='numPulses', ax=ax, color='red', label = 'numPulses- Outlier')
plt.title('[نمایش پراکندگی]')
plt.show()

#   نمایش پراکندگی وقتی 
# log scale=true
ax =  df_no_outliers.plot.scatter(x='id', y='numPulses', label = 'numPulses', logy=True)
df_out_outliers.plot.scatter(x='id', y='numPulses', ax=ax, color='red', label = 'numPulses- Outlier', logy =True)
plt.title('logscale=Trueنمایش پراکندگی')
plt.show()

"""**Decision Tree Classifier**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

classifier = DecisionTreeClassifier()
classifier.fit(X_train, y_train)

y_pred = classifier.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

scores_dict = {'svm' : [],'svm_less_features' : [], 'LR': [], 'LR_less_features': [], 'RF': []}

idx = np.arange(0, len(pd_speech_features) / 3).astype(int)
random_idx = random.sample(range(0, 251), int(len(pd_speech_features) * 0.80 / 3))

train_df = pd_speech_features[pd_speech_features.id.isin(random_idx)]
validation_df = pd_speech_features[np.logical_not(pd_speech_features.id.isin(random_idx))]

train_df_purged = pd_speech_features_no_tqwt[pd_speech_features_no_tqwt.id.isin(random_idx)]
validation_df_purged = pd_speech_features_no_tqwt[np.logical_not(pd_speech_features_no_tqwt.id.isin(random_idx))]

y_train = train_df['class']
y_train = np.array(y_train.values, dtype = 'int')
x_train = train_df.drop(['class'], axis = 1) 

y_validation = validation_df['class']
y_validation = np.array(y_validation.values, dtype = 'int')
x_validation = validation_df.drop(['class'], axis = 1)

"""**SVM Classifier (with and without cv)**"""

from sklearn import svm 
from sklearn.model_selection import cross_val_score
y_train = train_df['class']
y_train = np.array(y_train.values, dtype = 'int')
x_train = train_df.drop(['class'], axis = 1) 

y_validation = validation_df['class']
y_validation = np.array(y_validation.values, dtype = 'int')
x_validation = validation_df.drop(['class'], axis = 1) 

clf = svm.SVC(gamma = 0.0001, C=1.0, random_state=None, tol=1e-8)
clf.fit(x_train, y_train)  # no Cross Validation

scores = cross_val_score(clf, x_train, y_train, cv=5) # with 5 fold CV
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

y_prediction = clf.predict(x_validation) 
pred_score = clf.score(x_validation, y_validation)
pred_score

#Confusion Matrics
def compute_confusion_matrix(true, pred):
    K = len(np.unique(true)) # Number of classes 
    result = np.zeros((K, K))

    for i in range(len(true)):
        result[true[i]][pred[i]] += 1

    return result

def compute_accuracy(confusion_matrix, epsilon = 1e-7):
    TP, FP, TN, FN = confusion_matrix_decomposition(confusion_matrix)
    return (TP+TN) / (TP+TN+FP+FN+epsilon) 

def compute_precision(confusion_matrix, epsilon = 1e-7):
    TP, FP, TN, FN = confusion_matrix_decomposition(confusion_matrix)
    return TP / (FP+TP+epsilon)

def compute_f1(confusion_matrix, epsilon = 1e-7):
    TP, FP, TN, FN = confusion_matrix_decomposition(confusion_matrix)
    return  (2*TP) / (2*TP+FP + FN+epsilon) 

def compute_recall(confusion_matrix, epsilon = 1e-7):
    TP, FP, TN, FN = confusion_matrix_decomposition(confusion_matrix)
    return TP / (TP + FN+epsilon)

def confusion_matrix_decomposition(confusion_matrix): 
    TP = confusion_matrix[0,0]
    FP = confusion_matrix[0,1]
    TN = confusion_matrix[1,1]
    FN = confusion_matrix[1,0]
    
    return TP, FP, TN, FN

confusion_matrix = confusion_matrix(y_validation, y_prediction.astype('int'))
print(confusion_matrix)
print('testing accuracy {}'.format(pred_score))

"""accuracy, precision, F1, recall:"""

accuracy = compute_accuracy(confusion_matrix)
precision = compute_precision(confusion_matrix)
F1 =  compute_f1(confusion_matrix)
recall = compute_recall(confusion_matrix)

print('accuracy', accuracy)
print('precision', precision)
print('F1', F1)
print('recall', recall)

scores_dict['svm'] = [accuracy,precision, F1, recall]

"""**Random Forest** ***classifier***"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy') 
classifier.fit(x_train, y_train) 

# predicting the test set result
y_pred_Rforest = classifier.predict(x_validation)

classifier.score(x_validation, y_validation)

scores = cross_val_score(classifier, x_train, y_train, cv=5) # k=5 fold CV
print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2))

max_depth = np.linspace(1, 10, 10).astype(int)
n_estimators = np.linspace(1, 10, 10).astype(int)

accuracy_depth = []
precision_depth = []
f1_score_depth = []
recall_depth = []
    
for depth in tqdm(max_depth): 
    accuracy_estimator = []
    precision_estimator = []
    f1_score_estimator = []
    recall_estimator = []

    for estimator in n_estimators:
              
        classifier = RandomForestClassifier(criterion = 'entropy', max_depth = depth, n_estimators = estimator)
        classifier.fit(x_train, y_train)

        y_pred_Rforest = classifier.predict(x_validation)
        confusion_matrix =compute_confusion_matrix(y_validation, y_pred_Rforest.astype('int'))

        compute_accuracy(confusion_matrix)
        compute_precision(confusion_matrix)
        compute_f1(confusion_matrix)
        compute_recall(confusion_matrix)
            
    accuracy_depth.append(accuracy)
    precision_depth.append(precision)
    f1_score_depth.append(f1_score)
    recall_depth.append(recall)

y_pred_Rforest = classifier.predict(x_validation)

classifier.score(x_validation, y_validation)

print(compute_confusion_matrix(y_validation, y_pred_Rforest.astype('int')))
print('Accuracy = ', accuracy_depth)
print('Precision = ', precision_depth)
print('Recall = ', recall_depth)
print('F1 score = ', f1_score_depth)

"""**مهم ترین ویژگیها بر اساس معیار جینی**"""

feat_labels = train_df.columns

# نام ویژگی و میزان اهمیت ان بر اساس gini
for feature in zip(feat_labels, classifier.feature_importances_):
    print(feature)